"use strict";
/*
 * ATTENTION: An "eval-source-map" devtool has been used.
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file with attached SourceMaps in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
self["webpackHotUpdate_N_E"]("app/page",{

/***/ "(app-pages-browser)/./components/model/constants.ts":
/*!***************************************!*\
  !*** ./components/model/constants.ts ***!
  \***************************************/
/***/ (function(module, __webpack_exports__, __webpack_require__) {

eval(__webpack_require__.ts("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   LAYER_COLORS: function() { return /* binding */ LAYER_COLORS; },\n/* harmony export */   LAYER_EXPLANATIONS: function() { return /* binding */ LAYER_EXPLANATIONS; },\n/* harmony export */   LAYER_STATS: function() { return /* binding */ LAYER_STATS; }\n/* harmony export */ });\nconst LAYER_COLORS = {\n    input: \"#60a5fa\",\n    cnn: \"#3b82f6\",\n    transformer: \"#9333ea\",\n    rnn: \"#22c55e\",\n    output: \"#f43f5e\",\n    mlp: \"#8b5cf6\",\n    graph: \"#06b6d4\",\n    residual: \"#f59e0b\",\n    normalization: \"#10b981\",\n    attention: \"#ec4899\",\n    pooling: \"#0ea5e9\",\n    dropout: \"#6366f1\",\n    embedding: \"#d946ef\",\n    flatten: \"#14b8a6\"\n};\nconst LAYER_STATS = {\n    resnet: {\n        cnn: {\n            neurons: 9408,\n            inferenceTime: 1.2,\n            memoryUsage: 2.0,\n            filters: \"64→128→256\",\n            activations: \"ReLU\"\n        },\n        residual: {\n            neurons: 16384,\n            inferenceTime: 0.8,\n            memoryUsage: 3.0,\n            activations: \"ReLU\"\n        }\n    },\n    yolov8: {\n        cnn: {\n            neurons: 32768,\n            inferenceTime: 1.8,\n            memoryUsage: 2.0,\n            filters: \"128→256→512\",\n            activations: \"SiLU\"\n        },\n        residual: {\n            neurons: 32768,\n            inferenceTime: 1.2,\n            memoryUsage: 2.4,\n            activations: \"SiLU\"\n        }\n    },\n    transformer: {\n        attention: {\n            neurons: 49152,\n            inferenceTime: 1.8,\n            memoryUsage: 4.8,\n            heads: 8,\n            activations: \"Softmax\"\n        },\n        mlp: {\n            neurons: 3072,\n            inferenceTime: 0.8,\n            memoryUsage: 1.6,\n            activations: \"GELU\"\n        },\n        normalization: {\n            neurons: 768,\n            inferenceTime: 0.2,\n            memoryUsage: 0.4,\n            type: \"LayerNorm\"\n        }\n    },\n    gpt2: {\n        embedding: {\n            neurons: 50257 * 768,\n            inferenceTime: 0.3,\n            memoryUsage: 2.0,\n            activations: \"None\"\n        },\n        attention: {\n            neurons: 3 * 768 * 768,\n            inferenceTime: 2.0,\n            memoryUsage: 6.0,\n            heads: 12,\n            activations: \"Softmax\"\n        }\n    },\n    input: {\n        neurons: 150528,\n        inferenceTime: 0.1,\n        memoryUsage: 0.6,\n        activations: \"None\"\n    },\n    output: {\n        neurons: 1000,\n        inferenceTime: 0.2,\n        memoryUsage: 0.4,\n        activations: \"Softmax\"\n    },\n    rnn: {\n        neurons: 8192,\n        inferenceTime: 2.0,\n        memoryUsage: 3.2,\n        hiddenUnits: 512,\n        activations: \"Tanh\"\n    },\n    graph: {\n        neurons: 1024,\n        inferenceTime: 1.8,\n        memoryUsage: 2.4,\n        activations: \"ReLU\",\n        aggregation: \"Mean\"\n    },\n    normalization: {\n        neurons: 512,\n        inferenceTime: 0.2,\n        memoryUsage: 0.4,\n        type: \"BatchNorm\",\n        momentum: 0.99\n    }\n};\nconst LAYER_EXPLANATIONS = {\n    input: {\n        title: \"Input Layer\",\n        description: \"This layer processes raw input data (224\\xd7224 RGB images) and prepares it for the neural network. It applies normalization to scale pixel values between -1 and 1, making the data more suitable for deep learning.\",\n        technical: [\n            \"Input Shape: 224\\xd7224\\xd73\",\n            \"Normalization: [-1, 1]\",\n            \"Data Augmentation: Random crop, flip, rotation\"\n        ]\n    },\n    cnn: {\n        title: \"Convolutional Neural Network\",\n        description: \"The CNN block consists of 3 convolutional layers that progressively extract visual features. Each layer increases the number of filters while reducing spatial dimensions, allowing the network to learn hierarchical representations.\",\n        technical: [\n            \"3 Conv Layers: 64→128→256 filters\",\n            \"Kernel Size: 3\\xd73, Stride: 2\",\n            \"BatchNorm + ReLU activation\"\n        ]\n    },\n    transformer: {\n        title: \"Transformer Block\",\n        description: \"This block uses self-attention mechanisms to capture global relationships in the feature space. The multi-head attention allows the model to focus on different aspects of the input simultaneously.\",\n        technical: [\n            \"6 Attention Heads\",\n            \"Hidden Dim: 512\",\n            \"MLP Ratio: 4\",\n            \"LayerNorm + GELU\"\n        ]\n    },\n    rnn: {\n        title: \"Recurrent Neural Network\",\n        description: \"The RNN block processes sequential features using bidirectional LSTM cells. This allows the model to capture temporal dependencies in both forward and backward directions.\",\n        technical: [\n            \"Bidirectional LSTM\",\n            \"256 Hidden Units\",\n            \"Dropout: 0.2\",\n            \"Skip Connections\"\n        ]\n    },\n    output: {\n        title: \"Output Layer\",\n        description: \"The final layer produces class probabilities for 1000 different categories. It uses softmax activation to ensure all probabilities sum to 1, with temperature scaling for better calibration.\",\n        technical: [\n            \"1000 Output Classes\",\n            \"Softmax Activation\",\n            \"Temperature: 0.7\",\n            \"Cross-Entropy Loss\"\n        ]\n    },\n    mlp: {\n        title: \"Multi-Layer Perceptron\",\n        description: \"A fully connected neural network that transforms features through multiple dense layers. Each layer applies a linear transformation followed by non-linear activation.\",\n        technical: [\n            \"3 Dense Layers: 4096→2048→1024\",\n            \"ReLU Activation\",\n            \"Dropout: 0.5\",\n            \"Xavier Initialization\"\n        ]\n    },\n    graph: {\n        title: \"Graph Neural Network\",\n        description: \"Processes data structured as graphs, where each node aggregates information from its neighbors. Useful for molecular structures, social networks, and other graph-based data.\",\n        technical: [\n            \"Mean Aggregation\",\n            \"Edge Features\",\n            \"2-hop Neighborhood\",\n            \"Graph Attention\"\n        ]\n    },\n    residual: {\n        title: \"Residual Block\",\n        description: \"Implements skip connections that allow information to bypass layers directly. This helps mitigate the vanishing gradient problem and enables training of very deep networks.\",\n        technical: [\n            \"Identity Mapping\",\n            \"2 Conv Layers\",\n            \"Pre-activation\",\n            \"1\\xd71 Bottleneck\"\n        ]\n    },\n    normalization: {\n        title: \"Normalization Layer\",\n        description: \"Stabilizes training by normalizing feature distributions. Adapts to the statistics of each mini-batch while maintaining a running average for inference.\",\n        technical: [\n            \"Batch Normalization\",\n            \"Momentum: 0.99\",\n            \"Epsilon: 1e-5\",\n            \"Affine Transform\"\n        ]\n    },\n    attention: {\n        title: \"Attention Block\",\n        description: \"Computes dynamic weights for feature relationships. Each attention head specializes in different aspects of the input, enabling the model to capture complex dependencies.\",\n        technical: [\n            \"4 Attention Heads\",\n            \"Scaled Dot-Product\",\n            \"Key/Query Dim: 64\",\n            \"Softmax Temperature: 1.0\"\n        ]\n    }\n};\n\n\n;\n    // Wrapped in an IIFE to avoid polluting the global scope\n    ;\n    (function () {\n        var _a, _b;\n        // Legacy CSS implementations will `eval` browser code in a Node.js context\n        // to extract CSS. For backwards compatibility, we need to check we're in a\n        // browser context before continuing.\n        if (typeof self !== 'undefined' &&\n            // AMP / No-JS mode does not inject these helpers:\n            '$RefreshHelpers$' in self) {\n            // @ts-ignore __webpack_module__ is global\n            var currentExports = module.exports;\n            // @ts-ignore __webpack_module__ is global\n            var prevExports = (_b = (_a = module.hot.data) === null || _a === void 0 ? void 0 : _a.prevExports) !== null && _b !== void 0 ? _b : null;\n            // This cannot happen in MainTemplate because the exports mismatch between\n            // templating and execution.\n            self.$RefreshHelpers$.registerExportsForReactRefresh(currentExports, module.id);\n            // A module can be accepted automatically based on its exports, e.g. when\n            // it is a Refresh Boundary.\n            if (self.$RefreshHelpers$.isReactRefreshBoundary(currentExports)) {\n                // Save the previous exports on update so we can compare the boundary\n                // signatures.\n                module.hot.dispose(function (data) {\n                    data.prevExports = currentExports;\n                });\n                // Unconditionally accept an update to this module, we'll check if it's\n                // still a Refresh Boundary later.\n                // @ts-ignore importMeta is replaced in the loader\n                module.hot.accept();\n                // This field is set when the previous version of this module was a\n                // Refresh Boundary, letting us know we need to check for invalidation or\n                // enqueue an update.\n                if (prevExports !== null) {\n                    // A boundary can become ineligible if its exports are incompatible\n                    // with the previous exports.\n                    //\n                    // For example, if you add/remove/change exports, we'll want to\n                    // re-execute the importing modules, and force those components to\n                    // re-render. Similarly, if you convert a class component to a\n                    // function, we want to invalidate the boundary.\n                    if (self.$RefreshHelpers$.shouldInvalidateReactRefreshBoundary(prevExports, currentExports)) {\n                        module.hot.invalidate();\n                    }\n                    else {\n                        self.$RefreshHelpers$.scheduleUpdate();\n                    }\n                }\n            }\n            else {\n                // Since we just executed the code for the module, it's possible that the\n                // new exports made it ineligible for being a boundary.\n                // We only care about the case when we were _previously_ a boundary,\n                // because we already accepted this update (accidental side effect).\n                var isNoLongerABoundary = prevExports !== null;\n                if (isNoLongerABoundary) {\n                    module.hot.invalidate();\n                }\n            }\n        }\n    })();\n//# sourceURL=[module]\n//# sourceMappingURL=data:application/json;charset=utf-8;base64,{"version":3,"file":"(app-pages-browser)/./components/model/constants.ts","mappings":";;;;;;AACO,MAAMA,eAAe;IAC1BC,OAAO;IACPC,KAAK;IACLC,aAAa;IACbC,KAAK;IACLC,QAAQ;IACRC,KAAK;IACLC,OAAO;IACPC,UAAU;IACVC,eAAe;IACfC,WAAW;IACXC,SAAS;IACTC,SAAS;IACTC,WAAW;IACXC,SAAS;AACX,EAAE;AAEK,MAAMC,cAAc;IACzBC,QAAQ;QACNd,KAAK;YACHe,SAAS;YACTC,eAAe;YACfC,aAAa;YACbC,SAAS;YACTC,aAAa;QACf;QACAb,UAAU;YACRS,SAAS;YACTC,eAAe;YACfC,aAAa;YACbE,aAAa;QACf;IACF;IACAC,QAAQ;QACNpB,KAAK;YACHe,SAAS;YACTC,eAAe;YACfC,aAAa;YACbC,SAAS;YACTC,aAAa;QACf;QACAb,UAAU;YACRS,SAAS;YACTC,eAAe;YACfC,aAAa;YACbE,aAAa;QACf;IACF;IACAlB,aAAa;QACXO,WAAW;YACTO,SAAS;YACTC,eAAe;YACfC,aAAa;YACbI,OAAO;YACPF,aAAa;QACf;QACAf,KAAK;YACHW,SAAS;YACTC,eAAe;YACfC,aAAa;YACbE,aAAa;QACf;QACAZ,eAAe;YACbQ,SAAS;YACTC,eAAe;YACfC,aAAa;YACbK,MAAM;QACR;IACF;IACAC,MAAM;QACJZ,WAAW;YACTI,SAAS,QAAQ;YACjBC,eAAe;YACfC,aAAa;YACbE,aAAa;QACf;QACAX,WAAW;YACTO,SAAS,IAAI,MAAM;YACnBC,eAAe;YACfC,aAAa;YACbI,OAAO;YACPF,aAAa;QACf;IACF;IACApB,OAAO;QACLgB,SAAS;QACTC,eAAe;QACfC,aAAa;QACbE,aAAa;IACf;IACAhB,QAAQ;QACNY,SAAS;QACTC,eAAe;QACfC,aAAa;QACbE,aAAa;IACf;IACAjB,KAAK;QACHa,SAAS;QACTC,eAAe;QACfC,aAAa;QACbO,aAAa;QACbL,aAAa;IACf;IACAd,OAAO;QACLU,SAAS;QACTC,eAAe;QACfC,aAAa;QACbE,aAAa;QACbM,aAAa;IACf;IACAlB,eAAe;QACbQ,SAAS;QACTC,eAAe;QACfC,aAAa;QACbK,MAAM;QACNI,UAAU;IACZ;AACF,EAAE;AAEK,MAAMC,qBAAqB;IAChC5B,OAAO;QACL6B,OAAO;QACPC,aAAa;QACbC,WAAW;YACT;YACA;YACA;SACD;IACH;IACA9B,KAAK;QACH4B,OAAO;QACPC,aAAa;QACbC,WAAW;YACT;YACA;YACA;SACD;IACH;IACA7B,aAAa;QACX2B,OAAO;QACPC,aAAa;QACbC,WAAW;YACT;YACA;YACA;YACA;SACD;IACH;IACA5B,KAAK;QACH0B,OAAO;QACPC,aAAa;QACbC,WAAW;YACT;YACA;YACA;YACA;SACD;IACH;IACA3B,QAAQ;QACNyB,OAAO;QACPC,aAAa;QACbC,WAAW;YACT;YACA;YACA;YACA;SACD;IACH;IACA1B,KAAK;QACHwB,OAAO;QACPC,aAAa;QACbC,WAAW;YACT;YACA;YACA;YACA;SACD;IACH;IACAzB,OAAO;QACLuB,OAAO;QACPC,aAAa;QACbC,WAAW;YACT;YACA;YACA;YACA;SACD;IACH;IACAxB,UAAU;QACRsB,OAAO;QACPC,aAAa;QACbC,WAAW;YACT;YACA;YACA;YACA;SACD;IACH;IACAvB,eAAe;QACbqB,OAAO;QACPC,aAAa;QACbC,WAAW;YACT;YACA;YACA;YACA;SACD;IACH;IACAtB,WAAW;QACToB,OAAO;QACPC,aAAa;QACbC,WAAW;YACT;YACA;YACA;YACA;SACD;IACH;AACF,EAAE","sources":["webpack://_N_E/./components/model/constants.ts?61c5"],"sourcesContent":["\nexport const LAYER_COLORS = {\n  input: '#60a5fa',\n  cnn: '#3b82f6',\n  transformer: '#9333ea',\n  rnn: '#22c55e',\n  output: '#f43f5e',\n  mlp: '#8b5cf6',\n  graph: '#06b6d4',\n  residual: '#f59e0b',\n  normalization: '#10b981',\n  attention: '#ec4899',\n  pooling: '#0ea5e9',\n  dropout: '#6366f1',\n  embedding: '#d946ef',\n  flatten: '#14b8a6'\n};\n\nexport const LAYER_STATS = {\n  resnet: {\n    cnn: {\n      neurons: 9408,\n      inferenceTime: 1.2,\n      memoryUsage: 2.0,\n      filters: '64→128→256',\n      activations: 'ReLU'\n    },\n    residual: {\n      neurons: 16384,\n      inferenceTime: 0.8,\n      memoryUsage: 3.0,\n      activations: 'ReLU'\n    }\n  },\n  yolov8: {\n    cnn: {\n      neurons: 32768,\n      inferenceTime: 1.8,\n      memoryUsage: 2.0,\n      filters: '128→256→512',\n      activations: 'SiLU'\n    },\n    residual: {\n      neurons: 32768,\n      inferenceTime: 1.2,\n      memoryUsage: 2.4,\n      activations: 'SiLU'\n    }\n  },\n  transformer: {\n    attention: {\n      neurons: 49152,\n      inferenceTime: 1.8,\n      memoryUsage: 4.8,\n      heads: 8,\n      activations: 'Softmax'\n    },\n    mlp: {\n      neurons: 3072,\n      inferenceTime: 0.8,\n      memoryUsage: 1.6,\n      activations: 'GELU'\n    },\n    normalization: {\n      neurons: 768,\n      inferenceTime: 0.2,\n      memoryUsage: 0.4,\n      type: 'LayerNorm'\n    }\n  },\n  gpt2: {\n    embedding: {\n      neurons: 50257 * 768,\n      inferenceTime: 0.3,\n      memoryUsage: 2.0,\n      activations: 'None'\n    },\n    attention: {\n      neurons: 3 * 768 * 768,\n      inferenceTime: 2.0,\n      memoryUsage: 6.0,\n      heads: 12,\n      activations: 'Softmax'\n    }\n  },\n  input: {\n    neurons: 150528,\n    inferenceTime: 0.1,\n    memoryUsage: 0.6,\n    activations: 'None'\n  },\n  output: {\n    neurons: 1000,\n    inferenceTime: 0.2,\n    memoryUsage: 0.4,\n    activations: 'Softmax'\n  },\n  rnn: {\n    neurons: 8192,\n    inferenceTime: 2.0,\n    memoryUsage: 3.2,\n    hiddenUnits: 512,\n    activations: 'Tanh'\n  },\n  graph: {\n    neurons: 1024,\n    inferenceTime: 1.8,\n    memoryUsage: 2.4,\n    activations: 'ReLU',\n    aggregation: 'Mean'\n  },\n  normalization: {\n    neurons: 512,\n    inferenceTime: 0.2,\n    memoryUsage: 0.4,\n    type: 'BatchNorm',\n    momentum: 0.99\n  }\n};\n\nexport const LAYER_EXPLANATIONS = {\n  input: {\n    title: \"Input Layer\",\n    description: \"This layer processes raw input data (224×224 RGB images) and prepares it for the neural network. It applies normalization to scale pixel values between -1 and 1, making the data more suitable for deep learning.\",\n    technical: [\n      \"Input Shape: 224×224×3\",\n      \"Normalization: [-1, 1]\",\n      \"Data Augmentation: Random crop, flip, rotation\"\n    ]\n  },\n  cnn: {\n    title: \"Convolutional Neural Network\",\n    description: \"The CNN block consists of 3 convolutional layers that progressively extract visual features. Each layer increases the number of filters while reducing spatial dimensions, allowing the network to learn hierarchical representations.\",\n    technical: [\n      \"3 Conv Layers: 64→128→256 filters\",\n      \"Kernel Size: 3×3, Stride: 2\",\n      \"BatchNorm + ReLU activation\"\n    ]\n  },\n  transformer: {\n    title: \"Transformer Block\",\n    description: \"This block uses self-attention mechanisms to capture global relationships in the feature space. The multi-head attention allows the model to focus on different aspects of the input simultaneously.\",\n    technical: [\n      \"6 Attention Heads\",\n      \"Hidden Dim: 512\",\n      \"MLP Ratio: 4\",\n      \"LayerNorm + GELU\"\n    ]\n  },\n  rnn: {\n    title: \"Recurrent Neural Network\",\n    description: \"The RNN block processes sequential features using bidirectional LSTM cells. This allows the model to capture temporal dependencies in both forward and backward directions.\",\n    technical: [\n      \"Bidirectional LSTM\",\n      \"256 Hidden Units\",\n      \"Dropout: 0.2\",\n      \"Skip Connections\"\n    ]\n  },\n  output: {\n    title: \"Output Layer\",\n    description: \"The final layer produces class probabilities for 1000 different categories. It uses softmax activation to ensure all probabilities sum to 1, with temperature scaling for better calibration.\",\n    technical: [\n      \"1000 Output Classes\",\n      \"Softmax Activation\",\n      \"Temperature: 0.7\",\n      \"Cross-Entropy Loss\"\n    ]\n  },\n  mlp: {\n    title: \"Multi-Layer Perceptron\",\n    description: \"A fully connected neural network that transforms features through multiple dense layers. Each layer applies a linear transformation followed by non-linear activation.\",\n    technical: [\n      \"3 Dense Layers: 4096→2048→1024\",\n      \"ReLU Activation\",\n      \"Dropout: 0.5\",\n      \"Xavier Initialization\"\n    ]\n  },\n  graph: {\n    title: \"Graph Neural Network\",\n    description: \"Processes data structured as graphs, where each node aggregates information from its neighbors. Useful for molecular structures, social networks, and other graph-based data.\",\n    technical: [\n      \"Mean Aggregation\",\n      \"Edge Features\",\n      \"2-hop Neighborhood\",\n      \"Graph Attention\"\n    ]\n  },\n  residual: {\n    title: \"Residual Block\",\n    description: \"Implements skip connections that allow information to bypass layers directly. This helps mitigate the vanishing gradient problem and enables training of very deep networks.\",\n    technical: [\n      \"Identity Mapping\",\n      \"2 Conv Layers\",\n      \"Pre-activation\",\n      \"1×1 Bottleneck\"\n    ]\n  },\n  normalization: {\n    title: \"Normalization Layer\",\n    description: \"Stabilizes training by normalizing feature distributions. Adapts to the statistics of each mini-batch while maintaining a running average for inference.\",\n    technical: [\n      \"Batch Normalization\",\n      \"Momentum: 0.99\",\n      \"Epsilon: 1e-5\",\n      \"Affine Transform\"\n    ]\n  },\n  attention: {\n    title: \"Attention Block\",\n    description: \"Computes dynamic weights for feature relationships. Each attention head specializes in different aspects of the input, enabling the model to capture complex dependencies.\",\n    technical: [\n      \"4 Attention Heads\",\n      \"Scaled Dot-Product\",\n      \"Key/Query Dim: 64\",\n      \"Softmax Temperature: 1.0\"\n    ]\n  }\n};\n"],"names":["LAYER_COLORS","input","cnn","transformer","rnn","output","mlp","graph","residual","normalization","attention","pooling","dropout","embedding","flatten","LAYER_STATS","resnet","neurons","inferenceTime","memoryUsage","filters","activations","yolov8","heads","type","gpt2","hiddenUnits","aggregation","momentum","LAYER_EXPLANATIONS","title","description","technical"],"sourceRoot":""}\n//# sourceURL=webpack-internal:///(app-pages-browser)/./components/model/constants.ts\n"));

/***/ })

});